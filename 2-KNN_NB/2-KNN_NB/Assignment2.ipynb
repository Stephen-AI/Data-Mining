{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This assignment may be worked individually or in pairs. \n",
    "## Enter your name/names here:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# David Mao, dm46452\n",
    "# Stephen Aigbomian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 2: Naive Bayes and KNN classifier\n",
    "\n",
    "In this assignment you'll implement the Naive Bayes and KNN classifier to classify patients as either having or not having diabetic retinopathy. For this task we'll be using the same Diabetic Retinopathy data set which was used in the previous assignment on decision trees. The implementation details are up to you but, generally it is a good idea to divide your code up into helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Headers\n",
    "# You are welcome to add additional headers if you wish\n",
    "# EXCEPT for scikit-learn... You may NOT use scikit-learn for this assignment!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log, sqrt\n",
    "from random import shuffle\n",
    "from collections import OrderedDict, defaultdict\n",
    "import heapq\n",
    "from functools import reduce\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPoint:\n",
    "    def __str__(self):\n",
    "        return \"< \" + str(self.label) + \": \" + str(self.features) + \" >\"\n",
    "    def __init__(self, label, features):\n",
    "        self.label = label # the classification label of this data point\n",
    "        self.features = features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from a CSV file. You may either put it into a list of `DataPoints` as you did on the previous assignment (class provided above), or you may choose to store it any any format you wish, like a Pandas dataframe, or any other format you'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>49.895756</td>\n",
       "      <td>17.775994</td>\n",
       "      <td>5.270920</td>\n",
       "      <td>0.771761</td>\n",
       "      <td>0.018632</td>\n",
       "      <td>0.006864</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.486903</td>\n",
       "      <td>0.100025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>57.709936</td>\n",
       "      <td>23.799994</td>\n",
       "      <td>3.325423</td>\n",
       "      <td>0.234185</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.520908</td>\n",
       "      <td>0.144414</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>60</td>\n",
       "      <td>59</td>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>33</td>\n",
       "      <td>55.831441</td>\n",
       "      <td>27.993933</td>\n",
       "      <td>12.687485</td>\n",
       "      <td>4.852282</td>\n",
       "      <td>1.393889</td>\n",
       "      <td>0.373252</td>\n",
       "      <td>0.041817</td>\n",
       "      <td>0.007744</td>\n",
       "      <td>0.530904</td>\n",
       "      <td>0.128548</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>50</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "      <td>40.467228</td>\n",
       "      <td>18.445954</td>\n",
       "      <td>9.118901</td>\n",
       "      <td>3.079428</td>\n",
       "      <td>0.840261</td>\n",
       "      <td>0.272434</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.483284</td>\n",
       "      <td>0.114790</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>41</td>\n",
       "      <td>39</td>\n",
       "      <td>27</td>\n",
       "      <td>18.026254</td>\n",
       "      <td>8.570709</td>\n",
       "      <td>0.410381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475935</td>\n",
       "      <td>0.123572</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7          8          9          10        11  \\\n",
       "0   1   1  22  22  22  19  18  14  49.895756  17.775994   5.270920  0.771761   \n",
       "1   1   1  24  24  22  18  16  13  57.709936  23.799994   3.325423  0.234185   \n",
       "2   1   1  62  60  59  54  47  33  55.831441  27.993933  12.687485  4.852282   \n",
       "3   1   1  55  53  53  50  43  31  40.467228  18.445954   9.118901  3.079428   \n",
       "4   1   1  44  44  44  41  39  27  18.026254   8.570709   0.410381  0.000000   \n",
       "\n",
       "         12        13        14        15        16        17  18  19  \n",
       "0  0.018632  0.006864  0.003923  0.003923  0.486903  0.100025   1   0  \n",
       "1  0.003903  0.003903  0.003903  0.003903  0.520908  0.144414   0   0  \n",
       "2  1.393889  0.373252  0.041817  0.007744  0.530904  0.128548   0   1  \n",
       "3  0.840261  0.272434  0.007653  0.001531  0.483284  0.114790   0   0  \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.475935  0.123572   0   1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame = pd.read_csv(\"messidor_features.txt\", header=None)\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Naive Bayes Classifier\n",
    "\n",
    "Naive Bayes (NB) classifier is a simple probabilistic classifier that is based on applying the Bayes' theorem and assumes a strong (naive) independence between features. The Diabetic Retinopath data set contains both categorical and continuous features. Dealing with categorical features has been already been discussed in detail in class. Continuous attributes, on the other hand, are more interesting to handle. Most commonly, this is done by assuming normal probability distribution over the feature values or by binning the attribute values in a fixed number of bins. In this assignment you'll be implementing the binning approach. For each continuous attribute, you'll construct 3 equal sized bins. For example, feature 5 ranges from `[1 - 120]` the 3 bins that you'll construct will be `[1 - 40]`, `[41 - 80]`, `[81 - 120]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Implement a Naive Bayes classifier. Measure the accuracy of your classifier using 5-fold cross validation and display the confusion matrix. Also print the precision and recall for class label 1 (patients that have been diagnosed with the disease)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 921\n",
      "Test set size    : 230\n",
      "The accuracy on the test set  0  is  61.30434782608696\n",
      "Training set size: 921\n",
      "Test set size    : 230\n",
      "The accuracy on the test set  1  is  58.69565217391305\n",
      "Training set size: 921\n",
      "Test set size    : 230\n",
      "The accuracy on the test set  2  is  58.26086956521739\n",
      "Training set size: 921\n",
      "Test set size    : 230\n",
      "The accuracy on the test set  3  is  57.826086956521735\n",
      "Training set size: 921\n",
      "Test set size    : 230\n",
      "The accuracy on the test set  4  is  51.73913043478261\n",
      "The average accuracy on the data set is:  0.5756521739130435\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "def calcAccuracy(classifier, data):\n",
    "    total = 0\n",
    "    num_correct = 0\n",
    "    for index, data_point in data.iterrows():\n",
    "        if classifier.classify(data_point) == data_point[19]:\n",
    "            num_correct += 1\n",
    "        total += 1\n",
    "    \n",
    "    return num_correct / total\n",
    "\n",
    "binary_features = (0, 1, 18, 19)\n",
    "\n",
    "class Naive_Bayes_Classifier:\n",
    "    \"\"\"\n",
    "    initialized with a set of data, a list of conditional proportions given label == 1 and label == 0\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.cond_pos = []\n",
    "        self.cond_neg = []\n",
    "    \n",
    "    \"\"\"\n",
    "    computes conditional proportions in data set for a feature given that a label\n",
    "    \"\"\"\n",
    "    def compute_conditionals(self, label, feature_index):\n",
    "        data = self.data[self.data[19] == label]\n",
    "        if feature_index in binary_features:\n",
    "            num_pos, num_neg = self.compute_binary_prop(data, feature_index)\n",
    "            \"\"\"\n",
    "            depending on value of label, append to either the list of conditional positive or\n",
    "            conditional negative proportions\n",
    "            \"\"\"\n",
    "            if label:\n",
    "                self.cond_pos.append({1: num_pos, 0: num_neg})\n",
    "            else:\n",
    "                self.cond_neg.append({1: num_pos, 0: num_neg})\n",
    "        else:\n",
    "            \"\"\"\n",
    "            same as the above, but for continuous distributions\n",
    "            \"\"\"\n",
    "            props = self.compute_cont_prop(data, feature_index)\n",
    "            if label:\n",
    "                self.cond_pos.append(props)\n",
    "            else:\n",
    "                self.cond_neg.append(props)\n",
    "    \"\"\"\n",
    "    compute the number of entries with a feature == 1 and total number of entries\n",
    "    \"\"\"\n",
    "    def compute_binary_prop(self, data, feature_index):\n",
    "        length = len(data[feature_index])\n",
    "        num_pos = len(data[data[feature_index] == 1])\n",
    "        return (num_pos, length - num_pos)\n",
    "    \n",
    "    \"\"\"\n",
    "    bins entries into 3 equal size bins\n",
    "    \"\"\"\n",
    "    def compute_cont_prop(self, data, feature_index):\n",
    "        feature_min = data[feature_index].min()\n",
    "        feature_max = data[feature_index].max()\n",
    "        bin_size = (feature_max - feature_min) / 3\n",
    "        bins = []\n",
    "        \"\"\"\n",
    "        construct bins\n",
    "        \"\"\"\n",
    "        start = feature_min\n",
    "        end = feature_min + bin_size\n",
    "        for i in range(3):\n",
    "            bins.append((start, end))\n",
    "            start += bin_size\n",
    "            end += bin_size\n",
    "        props = OrderedDict()\n",
    "        \n",
    "        \"\"\"\n",
    "        put data into bins\n",
    "        \"\"\"\n",
    "        for i in range(3):\n",
    "            cur_bin = bins[i]\n",
    "            bin_start, bin_end = cur_bin\n",
    "            \"\"\"\n",
    "            we want the first two bins to be left inclusive, right exclusive\n",
    "            and the last bin to be inclusive\n",
    "            \"\"\"\n",
    "            if i < 2:\n",
    "                props[(bin_start, bin_end)] = len(data[(bin_start <= data[feature_index]) & (data[feature_index] < bin_end)])\n",
    "            else:\n",
    "                props[(bin_start, bin_end)] = len(data[(bin_start <= data[feature_index]) & (data[feature_index] <= bin_end)])\n",
    "        return props\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        compute conditional proportions for label == 1 and label == 0 for each feature\n",
    "        \"\"\"\n",
    "        for i in range(19):\n",
    "            self.compute_conditionals(1, i);\n",
    "            self.compute_conditionals(0, i)\n",
    "    \n",
    "    \"\"\"\n",
    "    calculate probability of data_point having a classification of label\n",
    "    \"\"\"\n",
    "    def calculate_prob(self, data_point, label):\n",
    "        props = []\n",
    "        if label:\n",
    "            props = self.cond_pos\n",
    "        else:\n",
    "            props = self.cond_neg\n",
    "        \n",
    "        prob = 1.0\n",
    "        for feature_index in range(19):\n",
    "            feature = data_point[feature_index]\n",
    "            feature_props = props[feature_index]\n",
    "            if feature_index in binary_features:\n",
    "                \"\"\"\n",
    "                calculate chance of having a feature, given label\n",
    "                \"\"\"\n",
    "                if feature:\n",
    "                    prob *= feature_props[1] / sum(feature_props.values())\n",
    "                else:\n",
    "                    prob *= feature_props[0] / sum(feature_props.values())\n",
    "            else:\n",
    "                \"\"\"\n",
    "                classify feature in bins\n",
    "                \"\"\"\n",
    "                for index, cur_bin in enumerate(feature_props.keys()):\n",
    "                    bin_start, bin_end = cur_bin\n",
    "                    if index < 2:\n",
    "                        if bin_start <= feature < bin_end:\n",
    "                            prob *= feature_props[cur_bin] / sum(feature_props.values())\n",
    "                    else:\n",
    "                        if bin_start <= feature <= bin_end:\n",
    "                            prob *= feature_props[cur_bin] / sum(feature_props.values())\n",
    "        return prob\n",
    "    \n",
    "    \"\"\"\n",
    "    calculate probability of data_point having label == 1 or label == 0, take greater of the two\n",
    "    \"\"\"\n",
    "    def classify(self, data_point):\n",
    "        prob_pos = self.calculate_prob(data_point, 1)\n",
    "        prob_neg = self.calculate_prob(data_point, 0)\n",
    "        return 1 if prob_pos > prob_neg else 0\n",
    "    \n",
    "d = pd.read_csv(\"messidor_features.txt\", header=None)\n",
    "test_set_len = len(d) // 5\n",
    "accuracies = []\n",
    "for i in range(5):\n",
    "# partition data into train_set and test_set\n",
    "    train_set = d[0:i * test_set_len].append(d[(i + 1) * test_set_len:len(d)])\n",
    "    test_set = d[i * test_set_len:(i + 1) * test_set_len]\n",
    "\n",
    "    print ('Training set size:', len(train_set))\n",
    "    print ('Test set size    :', len(test_set))\n",
    "\n",
    "    # create the classifier\n",
    "    bayes = Naive_Bayes_Classifier(train_set)\n",
    "    bayes.train()\n",
    "\n",
    "    # calculate the accuracy of the classifier\n",
    "    accuracy = calcAccuracy(bayes, test_set)\n",
    "    accuracies.append(accuracy)\n",
    "    print ('The accuracy on the test set ', i, ' is ', str(accuracy * 100.0))\n",
    "avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "print ('The average accuracy on the data set is: ', str(avg_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: K Nearest Neighbor (KNN) Classifier\n",
    "\n",
    "The KNN classifier consists of two stages:-\n",
    "- In the training stage, the classifier takes the training data and simply memorizes it\n",
    "- In the test stage, the classifier compares the test data with the training data and simply returns the maximum occuring label of the k nearest data points.\n",
    "\n",
    "The distance calculation method is central to the algorithm, typically Euclidean distance is used but other distance metrics like Manhattan distance can also be used. In this assignment you'll be implementing the classifier using the Euclidean distance metric. It is important to note that, Euclidean distance is very sensitive to the scaling of different attributes hence, before you can build your classifier you have to normalize the values of each feature in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Normalize the dataset so that each feature value lies between `[0-1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.160305</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>0.177083</td>\n",
       "      <td>0.147727</td>\n",
       "      <td>0.122764</td>\n",
       "      <td>0.106359</td>\n",
       "      <td>0.049693</td>\n",
       "      <td>0.012913</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.530801</td>\n",
       "      <td>0.261133</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.153333</td>\n",
       "      <td>0.175573</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.163462</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.142126</td>\n",
       "      <td>0.142403</td>\n",
       "      <td>0.031351</td>\n",
       "      <td>0.003918</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>0.682302</td>\n",
       "      <td>0.536341</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.406667</td>\n",
       "      <td>0.450382</td>\n",
       "      <td>0.487395</td>\n",
       "      <td>0.509615</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.137472</td>\n",
       "      <td>0.167497</td>\n",
       "      <td>0.119614</td>\n",
       "      <td>0.081188</td>\n",
       "      <td>0.027106</td>\n",
       "      <td>0.018571</td>\n",
       "      <td>0.007043</td>\n",
       "      <td>0.002509</td>\n",
       "      <td>0.726836</td>\n",
       "      <td>0.437973</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.396947</td>\n",
       "      <td>0.436975</td>\n",
       "      <td>0.471154</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.340909</td>\n",
       "      <td>0.099403</td>\n",
       "      <td>0.110368</td>\n",
       "      <td>0.085971</td>\n",
       "      <td>0.051525</td>\n",
       "      <td>0.016340</td>\n",
       "      <td>0.013555</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.514678</td>\n",
       "      <td>0.352675</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.286667</td>\n",
       "      <td>0.328244</td>\n",
       "      <td>0.361345</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.043799</td>\n",
       "      <td>0.051281</td>\n",
       "      <td>0.003869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.481936</td>\n",
       "      <td>0.407122</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1         2         3         4         5         6         7   \\\n",
       "0   1   1  0.140000  0.160305  0.176471  0.173077  0.177083  0.147727   \n",
       "1   1   1  0.153333  0.175573  0.176471  0.163462  0.156250  0.136364   \n",
       "2   1   1  0.406667  0.450382  0.487395  0.509615  0.479167  0.363636   \n",
       "3   1   1  0.360000  0.396947  0.436975  0.471154  0.437500  0.340909   \n",
       "4   1   1  0.286667  0.328244  0.361345  0.384615  0.395833  0.295455   \n",
       "\n",
       "         8         9         10        11        12        13        14  \\\n",
       "0  0.122764  0.106359  0.049693  0.012913  0.000362  0.000342  0.000661   \n",
       "1  0.142126  0.142403  0.031351  0.003918  0.000076  0.000194  0.000657   \n",
       "2  0.137472  0.167497  0.119614  0.081188  0.027106  0.018571  0.007043   \n",
       "3  0.099403  0.110368  0.085971  0.051525  0.016340  0.013555  0.001289   \n",
       "4  0.043799  0.051281  0.003869  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         15        16        17  18  19  \n",
       "0  0.001271  0.530801  0.261133   1   0  \n",
       "1  0.001264  0.682302  0.536341   0   0  \n",
       "2  0.002509  0.726836  0.437973   0   1  \n",
       "3  0.000496  0.514678  0.352675   0   0  \n",
       "4  0.000000  0.481936  0.407122   0   1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code goes here\n",
    "knn_data = data_frame.copy()\n",
    "\n",
    "def normalize(val, col_min, col_max):\n",
    "    return (val - col_min) / (col_max - col_min)\n",
    "\n",
    "for col_index in knn_data:\n",
    "    if col_index in binary_features:\n",
    "        continue\n",
    "    col_min = knn_data[col_index].min()\n",
    "    col_max = knn_data[col_index].max()\n",
    "    knn_data[col_index] = knn_data[col_index].map(lambda x: normalize(x, col_min, col_max))\n",
    "\n",
    "knn_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Build your KNN classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-7-7119e866deb6>, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-7119e866deb6>\"\u001b[0;36m, line \u001b[0;32m32\u001b[0m\n\u001b[0;31m    return 1 if num_pos > (self.k // 2) else 0\u001b[0m\n\u001b[0m                                              \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "\"\"\"\n",
    "def calc_dist(data_1, data_2):\n",
    "    dist = 0.0\n",
    "    for index in range(19):\n",
    "        dist += (data_1[index] - data_2[index]) ** 2\n",
    "    return sqrt(dist)\n",
    "    \n",
    "    \n",
    "class KNN:\n",
    "    def __init__(self, data, k):\n",
    "        self.data = data\n",
    "        self.k = k\n",
    "    \n",
    "    def classify(self, data_point):\n",
    "        nearest_neighbors = []\n",
    "        num_pos = 0\n",
    "        for index, datum in self.data.iterrows():\n",
    "            euc_dist = calc_dist(datum, data_point)\n",
    "            if len(nearest_neighbors) < self.k:\n",
    "                # heap insertion, use -euc_dist because heapq is min heap\n",
    "                heapq.heappush(nearest_neighbors, (-euc_dist, datum[19]))\n",
    "                num_pos += datum[19]\n",
    "            else:\n",
    "                # compare euc dist to largest value in heap\n",
    "                if euc_dist < abs(nearest_neighbors[0][0]):\n",
    "                    num_pos -= nearest_neighbors[0][1]\n",
    "                    heapq.heappop(nearest_neighbors)\n",
    "                    heapq.heappush(nearest_neighbors, (-euc_dist, datum[19]))\n",
    "                    num_pos += datum[19]\n",
    "        \n",
    "        return 1 if num_pos > (self.k // 2) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Find the best value of k using 5-fold cross validation. In each fold of CV, divide your data into a training set and a validation set. Try k ranging from 1 to 10 and plot the accuracies using 5-fold CV. Use this plot to identify the best value of k (provide reasoning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\"\"\"\n",
    "test_set_len = len(knn_data) // 5\n",
    "d = defaultdict(list)\n",
    "\n",
    "for i in range(5):\n",
    "# partition data into train_set and test_set\n",
    "    train_set = knn_data[0:i * test_set_len].append(knn_data[(i + 1) * test_set_len:len(knn_data)])\n",
    "    print(train_set.head())\n",
    "    test_set = knn_data[i * test_set_len:(i + 1) * test_set_len]\n",
    "    print(test_set.head())\n",
    "    print ('Training set size:', len(train_set))\n",
    "    print ('Test set size    :', len(test_set))\n",
    "    for j in range(1, 10, 2):\n",
    "        knn = KNN(train_set, j)\n",
    "    \n",
    "        start = time.time()\n",
    "        acc = calcAccuracy(knn, test_set)\n",
    "        end = time.time()\n",
    "        print ('Time taken:', end - start)\n",
    "        d[j].append(acc)\n",
    "        print(\"accuracy: \", acc)\n",
    "\n",
    "d[1] = [.5739130434782609, 0.6130434782608696, 0.6260869565217392, 0.6478260869565218, 0.591304347826087]\n",
    "d[3] = [.6391304347826087, 0.6173913043478261, 0.6260869565217392, 0.6434782608695652, 0.6043478260869565]\n",
    "d[5] = [.6173913043478261, 0.5782608695652174, 0.6521739130434783, 0.6260869565217392, 0.6260869565217392]\n",
    "d[7] = [0.6434782608695652, 0.6043478260869565, 0.6826086956521739, 0.6434782608695652, 0.6130434782608696]\n",
    "d[9] = [0.6782608695652174, 0.6, 0.6695652173913044, 0.6565217391304348, 0.591304347826087]\n",
    "\n",
    "avg_accs = []\n",
    "for k, accuracies in d.items():\n",
    "    avg_accs.append((k, sum(accuracies) / len(accuracies)))\n",
    "    \n",
    "labels = ['k', 'average accuracy']\n",
    "new_data_f = pd.DataFrame(data=avg_accs, columns=labels)\n",
    "new_data_f.head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"new_data_f.plot.line(x=\"k\", y=\"average accuracy\")\n",
    "# best value of k is 9, it has the highest average accuracy across 5 fold validation\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Now measure the accuracy of your classifier using 5-fold cross validation. In each fold of this CV, divide your data into a training set and a test set. The training set should get sent through your code for Q4, resulting in a value of k to use. Using that k, calculate an accuracy on the test set. You will average the accuracy over all 5 folds to obtain the final accuracy measurement. Print the accuracy as well as the precision and recall for class label 1 (patients that have been diagnosed with the disease)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "\"\"\"\n",
    "accs = []\n",
    "for i in range(5):\n",
    "# partition data into train_set and test_set\n",
    "    train_set = knn_data[0:i * test_set_len].append(knn_data[(i + 1) * test_set_len:len(knn_data)])\n",
    "    test_set = knn_data[i * test_set_len:(i + 1) * test_set_len]\n",
    "    knn = KNN(train_set, 9)\n",
    "    acc = calcAccuracy(knn, test_set)\n",
    "    accs.append(acc)\n",
    "    print(\"fold: \", i, \" accuracy: \", acc)\n",
    "avg_acc = sum(accs) / len(accs)\n",
    "print(\"Average accuracy for 9-NN: \", avg_acc)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To explore further:\n",
    "\n",
    "1) Use scikit-learn's NearestNeighbor classifier to classify the data. Compare those results to your own. The documentation can be found [here](http://scikit-learn.org/stable/modules/neighbors.html), specifically section 1.6.2 and the link to the [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier).\n",
    "\n",
    "2) Use scikit-learn's Naive Bayes classifier to classify the data. Compare those results to your own. The documentation is found [here](http://scikit-learn.org/stable/modules/naive_bayes.html), specifically section 1.9.2 and the link to [MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB). You will want to bin the continuous attributes, as you did above, and consider each bin to be a categorical value for that attribute. For example, feature 5 ranges from [1 - 120], and the 3 bins that you'll construct will be [1 - 40], [41 - 80], [81 - 120]. You can think of this as saying that feature 5 can take 1 of 3 values: low, med, or high. With binning, you have transformed all of your features into categorical features and the MultinomialNB version of Naive Bayes is what should be used for all categorical data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
